apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: load-hca
spec:
  entrypoint: main
  serviceAccountName: {{ .Values.serviceAccount.k8sName }}
  templates:
    - name: main
      inputs:
        parameters:
          - name: data-repo-name
            {{- $dataRepoName := "{{inputs.parameters.data-repo-name}}" }}
          - name: dataset-id
            {{- $datasetId := "{{inputs.parameters.dataset-id}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      # Limit the number of active processing "arms" at a time.
      # There may be multiple pods running within each arm.
      parallelism: {{ .Values.parallelism }}
      dag:
        tasks:
          {{- $datasetName := printf "%s_%s" .Values.bigquery.stagingData.datasetPrefix $stagingPrefix }}
          - name: create-staging-dataset
            templateRef:
              name: {{ .Values.argoTemplates.createBQDataset.name }}
              template: main
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $datasetName | quote }}
                {{- with .Values.bigquery.stagingData }}
                - name: bq-project
                  value: {{ .project }}
                - name: dataset-description
                  value: {{ .description }}
                - name: dataset-expiration
                  value: {{ .expiration | quote }}
                {{- end }}

          - name: ingest-non-file-metadata
            dependencies: [create-staging-dataset]
            withItems:
              - aggregate_generation_protocol
              - analysis_process
              - analysis_protocol
              - cell_line
              - cell_suspension
              - collection_protocol
              - differentiation_protocol
              - dissociation_protocol
              - donor_organism
              - enrichment_protocol
              - imaged_specimen
              - imaging_preparation_protocol
              - imaging_protocol
              - ipsc_induction_protocol
              - library_preparation_protocol
              - organoid
              - process
              - project
              - protocol
              - sequencing_protocol
              - specimen_from_organism
              - links
            templateRef:
              name: load-table
              template: main
            arguments:
              parameters:
                - name: table
                  value: {{ "{{item}}" | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: metadata-type
                  value: metadata
                - name: bq-dataset
                  value: {{ $datasetName | quote }}
                - name: data-repo-name
                  value: {{ $dataRepoName | quote }}
                - name: dataset-id
                  value: {{ $datasetId | quote }}

          {{- $inputFileLoadPrefix := printf "%s/data-transfer-requests" $stagingPrefix }}
          {{- $fileLoadTable := "file_load_requests" }}
          - name: diff-file-loads
            dependencies: [create-staging-dataset]
            template: diff-file-loads
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $inputFileLoadPrefix | quote }}
                - name: target-table
                  value: {{ $fileLoadTable }}
                - name: staging-bq-dataset
                  value: {{ $datasetName | quote }}
                - name: data-repo-name
                  value: {{ $dataRepoName | quote }}

          {{- $outputFileLoadPrefix := printf "%s/data-transfer-requests-deduped" $stagingPrefix }}
          - name: extract-file-loads
            dependencies: [diff-file-loads]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $datasetName | quote }}
                - name: bq-table
                  value: {{ $fileLoadTable }}
                - name: output-format
                  value: NEWLINE_DELIMITED_JSON
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
          {{- $totalLoadCount := "{{tasks.extract-file-loads.outputs.parameters.row-count}}" }}

          - name: list-deduped-requests
            dependencies: [extract-file-loads]
            template: list-gcs-contents
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
            {{- $fileLoads := "{{tasks.list-deduped-requests.outputs.result}}" }}

          - name: ingest-data-files
            dependencies: [list-deduped-requests, extract-file-loads]
            template: run-bulk-file-ingest
            withParam: {{ $fileLoads | quote }}
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ "{{item.path}}" | quote }}
                - name: control-file-index
                  value: {{ "{{item.id}}" | quote }}
                - name: load-tag-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: total-file-count
                  value: {{ $totalLoadCount | quote }}
                - name: dataset-id
                  value: {{ $datasetId | quote }}

          - name: ingest-file-metadata
            dependencies: [ingest-data-files]
            withItems:
              - analysis_file
              - image_file
              - reference_file
              - sequence_file
              - supplementary_file
            template: ingest-file-metadata
            arguments:
              parameters:
                - name: table
                  value: {{ "{{item}}" | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: bq-dataset
                  value: {{ $datasetName | quote }}
                - name: data-repo-name
                  value: {{ $dataRepoName | quote }}
                - name: dataset-id
                  value: {{ $datasetId | quote }}

    ##
    ## Diff the bulk file-load requests staged in GCS against the files already present in the repo.
    ##
    - name: diff-file-loads
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: target-table
          {{- $targetTable := "{{inputs.parameters.target-table}}" }}
          - name: staging-bq-dataset
          {{- $bqDataset := "{{inputs.parameters.staging-bq-dataset}}" }}
          - name: data-repo-name
      script:
        image: google/cloud-sdk:slim
        env:
          - name: GCS_PREFIX
            value: {{ printf "gs://%s/%s" .Values.gcs.stagingBucketName $gcsPrefix | quote }}
          - name: TARGET_TABLE
            value: {{ $targetTable | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ $dataRepoName | quote }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/diff-data-files.sh") | indent 10 }}

    ##
    ## List the contents of a GCS bucket under a prefix, returning the results as a JSON array
    ## so Argo can parse and scatter over them.
    ##
    - name: list-gcs-contents
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/gcs-python:1.0.0
        env:
          - name: GCS_BUCKET
            value: {{ .Values.gcs.stagingBucketName }}
          - name: GCS_PREFIX
            value: {{ $gcsPrefix | quote }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/list-gcs-files-as-array.py") | indent 10 }}

    ##
    ## Run a repo bulk-file ingest to completion.
    ##
    - name: run-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: control-file-index
          {{- $controlFileIndex := "{{inputs.parameters.control-file-index}}" }}
          - name: load-tag-prefix
          {{- $loadTagPrefix := "{{inputs.parameters.load-tag-prefix}}" }}
          - name: total-file-count
          {{- $totalFileCount := "{{inputs.parameters.total-file-count}}" }}
          - name: dataset-id
      dag:
        tasks:
          {{- $loadTag := printf "%s-%s" $loadTagPrefix $controlFileIndex }}
          - name: submit
            template: submit-bulk-file-ingest
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ $controlFilePath | quote }}
                - name: load-tag
                  value: {{ $loadTag | quote }}
                - name: max-failures
                  value: {{ $totalFileCount | quote }}
                - name: dataset-id
                  value: {{ $datasetId | quote }}
          {{- $jobId := "{{tasks.submit.outputs.result}}" }}

          - name: poll
            dependencies: [submit]
            template: poll-ingest-job
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}
                {{- with .Values.repo }}
                - name: api-url
                  value: {{ .url }}
                - name: timeout
                  value: {{ .pollTimeout | quote }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: check
            dependencies: [poll]
            template: check-bulk-file-ingest
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}
          - name: validate-checksums
            dependencies: [poll]
            template: validate-checksums
            arguments:
              parameters:
                - name: load-tag
                  value: {{ $loadTag | quote }}
          - name: move-error-logs-to-gcs
            dependencies: [validate-checksums]
            template: move-error-logs-to-gcs

    ##
    ## Submit a bulk-file ingest using a request staged in GCS.
    ##
    - name: submit-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: load-tag
          {{- $loadTag := "{{inputs.parameters.load-tag}}" }}
          - name: max-failures
          {{- $maxFailures := "{{inputs.parameters.max-failures}}" }}
          - name: dataset-id
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: INPUT_PATH
            value: {{ $controlFilePath | quote }}
          - name: LOAD_TAG
            value: {{ $loadTag | quote }}
          - name: MAX_FAILURES
            value: {{ $maxFailures | quote }}
          {{- with .Values.repo }}
          - name: API_URL
            value: {{ .url }}
          - name: DATASET_ID
            value: {{ $datasetId | quote }}
          - name: PROFILE_ID
            value: {{ .profileId }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .accessKey.secretKey }}
          {{- end }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/submit-bulk-file-ingest.py") | indent 10 }}

    ##
    ## Inject common template used to poll TDR jobs.
    ##
    {{- include "argo.poll-ingest-job" . | indent 4 }}

    ##
    ## Check that a bulk-file ingest completed without any failures.
    ##
    - name: check-bulk-file-ingest
      inputs:
        parameters:
          - name: job-id
          {{- $jobId := "{{inputs.parameters.job-id}}" }}
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: JOB_ID
            value: {{ $jobId | quote }}
          - name: API_URL
            value: {{ .Values.repo.url }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .Values.repo.accessKey.secretKey }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/check-bulk-file-ingest-result.py") | indent 10 }}

    ##
    ## Ingest metadata for a table containing information about data files.
    ##
    - name: ingest-file-metadata
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: staging-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-prefix}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
          - name: data-repo-name
          - name: dataset-id
      dag:
        tasks:
          - name: inject-file-ids
            template: inject-file-ids
            arguments:
              parameters:
                - name: table
                  value: {{ $table | quote }}
                - name: gcs-prefix
                  value: {{ printf "%s/metadata" $stagingPrefix | quote }}
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: data-repo-name
                  value: {{ $dataRepoName | quote }}
          {{- $fileIdsTable := "{{tasks.inject-file-ids.outputs.result}}" }}

          {{- $metadataPrefix := printf "%s/file-metadata-with-ids/%s" $stagingPrefix $table }}
          - name: export-metadata-with-ids
            dependencies: [inject-file-ids]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: bq-table
                  value: {{ $fileIdsTable | quote }}
                - name: output-format
                  value: NEWLINE_DELIMITED_JSON
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $metadataPrefix | quote }}
          {{- $rowCount := "{{tasks.export-metadata-with-ids.outputs.parameters.row-count}}" }}

          - name: ingest-metadata
            dependencies: [export-metadata-with-ids]
            when: {{ printf "%s > 0" $rowCount | quote }}
            templateRef:
              name: load-table
              template: main
            arguments:
              parameters:
                - name: table
                  value: {{ $table | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: metadata-type
                  value: file-metadata-with-ids
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: data-repo-name
                  value: {{ $dataRepoName | quote }}
                - name: dataset-id
                  value: {{ $datasetId | quote }}

    ##
    ## Rewrite staged tabular data for files to inject corresponding TDR file IDs.
    ##
    - name: inject-file-ids
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
          - name: data-repo-name
      script:
        image: google/cloud-sdk:slim
        env:
          - name: GCS_PREFIX
            value: {{ printf "gs://%s/%s/%s" .Values.gcs.stagingBucketName $gcsPrefix $table | quote }}
          - name: TABLE
            value: {{ $table | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ $dataRepoName | quote }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/inject-file-ids.sh") | indent 10 }}

    ##
    ## Validate that the file checksums have not changed after files were loaded into the data repo.
    ##
    - name: validate-checksums
      inputs:
        parameters:
          - name: load-tag
          {{- $loadTag := "{{inputs.parameters.load-tag}}" }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/gcs-python:1.0.0
        env:
          - name: PROJECT_ID
            value: {{ .Values.bigquery.jadeData.project }}
          - name: DATASET_NAME
            value: {{ .Values.bigquery.jadeData.dataset }}
          - name: LOAD_TAG
            value: {{ $loadTag | quote }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/validate-checksums.py") | indent 10 }}

    ##
    ## Move any logs to GCS.
    ##
    - name: move-error-logs-to-gcs
      script:
        image: google/cloud-sdk:slim
        env:
          - name: TIMESTAMP
            value: {{ include "argo.timestamp" . | quote }}
          - name: BUCKET_NAME
            value: {{ .Values.gcs.stagingBucketName }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/move-error-logs-to-gcs.sh") | indent 10 }}
