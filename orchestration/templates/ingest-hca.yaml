apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ingest-hca
spec:
  entrypoint: main
  serviceAccountName: {{ .Values.serviceAccount.k8sName }}
  templates:
    - name: main
      inputs:
        parameters:
          - name: source-bucket-name
          {{- $inputBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
          {{- $inputPrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      # Limit the number of active processing "arms" at a time.
      # There may be multiple pods running within each arm.
      parallelism: {{ .Values.parallelism }}
      dag:
        tasks:
          # Clear the staging directory to avoid corrupt state.
          - name: clear-staging-dir
            template: delete-gcs-directory
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $stagingPrefix | quote }}

          # Pre-process the metadata found in a source bucket
          - name: pre-process-metadata
            dependencies: [clear-staging-dir]
            template: run-dataflow
            arguments:
              parameters:
                - name: source-bucket-name
                  value: {{ $inputBucket | quote }}
                - name: source-bucket-prefix
                  value: {{ $inputPrefix | quote }}
                - name: staging-bucket-prefix
                  value: {{ $stagingPrefix | quote }}

          {{- $datasetName := printf "%s_%s" .Values.bigquery.stagingData.datasetPrefix $stagingPrefix }}
          - name: create-staging-dataset
            templateRef:
              name: {{ .Values.argoTemplates.createBQDataset.name }}
              template: main
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $datasetName | quote }}
                {{- with .Values.bigquery.stagingData }}
                - name: bq-project
                  value: {{ .project }}
                - name: dataset-description
                  value: {{ .description }}
                - name: dataset-expiration
                  value: {{ .expiration | quote }}
                {{- end }}

          - name: ingest-non-file-metadata
            dependencies: [pre-process-metadata, create-staging-dataset]
            withItems:
              - aggregate_generation_protocol
              - analysis_process
              - analysis_protocol
              - cell_line
              - cell_suspension
              - collection_protocol
              - differentiation_protocol
              - dissociation_protocol
              - donor_organism
              - enrichment_protocol
              - imaged_specimen
              - imaging_preparation_protocol
              - imaging_protocol
              - ipsc_induction_protocol
              - library_preparation_protocol
              - organoid
              - process
              - project
              - protocol
              - sequencing_protocol
              - specimen_from_organism
              - links
            template: ingest-metadata
            arguments:
              parameters:
                - name: table
                  value: {{ "{{item}}" | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: metadata-type
                  value: metadata
                - name: bq-dataset
                  value: {{ $datasetName | quote }}

          {{- $inputFileLoadPrefix := printf "%s/data-transfer-requests" $stagingPrefix }}
          {{- $fileLoadTable := "file_load_requests" }}
          - name: diff-file-loads
            dependencies: [pre-process-metadata, create-staging-dataset]
            template: diff-file-loads
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $inputFileLoadPrefix | quote }}
                - name: target-table
                  value: {{ $fileLoadTable }}
                - name: staging-bq-dataset
                  value: {{ $datasetName | quote }}

          {{- $outputFileLoadPrefix := printf "%s/data-transfer-requests-deduped" $stagingPrefix }}
          - name: extract-file-loads
            dependencies: [diff-file-loads]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $datasetName | quote }}
                - name: bq-table
                  value: {{ $fileLoadTable }}
                - name: output-format
                  value: NEWLINE_DELIMITED_JSON
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
          {{- $totalLoadCount := "{{tasks.extract-file-loads.outputs.parameters.row-count}}" }}

          - name: list-deduped-requests
            dependencies: [extract-file-loads]
            template: list-gcs-contents
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
            {{- $fileLoads := "{{tasks.list-deduped-requests.outputs.result}}" }}

          - name: ingest-data-files
            dependencies: [list-deduped-requests, extract-file-loads]
            template: run-bulk-file-ingest
            withParam: {{ $fileLoads | quote }}
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ "{{item.path}}" | quote }}
                - name: control-file-index
                  value: {{ "{{item.id}}" | quote }}
                - name: load-tag-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: total-file-count
                  value: {{ $totalLoadCount | quote }}

          - name: ingest-file-metadata
            dependencies: [ingest-data-files]
            withItems:
              - analysis_file
              - image_file
              - reference_file
              - sequence_file
              - supplementary_file
            template: ingest-file-metadata
            arguments:
              parameters:
                - name: table
                  value: {{ "{{item}}" | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: bq-dataset
                  value: {{ $datasetName | quote }}

    ##
    ## Delete all the files under a prefix in GCS.
    ##
    - name: delete-gcs-directory
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: google/cloud-sdk:slim
        {{- $fullPath := printf "gs://%s/%s/*" .Values.gcs.stagingBucketName $gcsPrefix }}
        command: [bash]
        source: |
          # rm will fail if there's already nothing at the target path.
          gsutil -m rm -r {{ $fullPath }} || true

    ##
    ## Run a Dataflow job to pre-process JSON metadata from the input area,
    ## moving it into our GCP space in the process.
    ##
    - name: run-dataflow
      inputs:
        parameters:
          - name: source-bucket-name
          {{- $sourceBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
          {{- $sourcePrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      container:
        {{- $version := default "latest" .Chart.AppVersion }}
        image: us.gcr.io/broad-dsp-gcr-public/hca-transformation-pipeline:{{ $version }}
        command: []
        args:
          - --runner=dataflow
          - --inputPrefix=gs://{{ $sourceBucket }}{{ $sourcePrefix }}
          - --outputPrefix=gs://{{ .Values.gcs.stagingBucketName }}/{{ $stagingPrefix }}
          {{- with .Values.dataflow }}
          - --project={{ .project }}
          - --region={{ .region }}
          - --tempLocation=gs://{{ .tmpBucketName }}/dataflow
          - --subnetwork=regions/{{ .region }}/subnetworks/{{ .subnetName }}
          - --serviceAccount={{ .workerAccount }}
          - --workerMachineType={{ .workerMachineType }}
          {{- with .autoscaling }}
          - --autoscalingAlgorithm=THROUGHPUT_BASED
          - --numWorkers={{ .minWorkers }}
          - --maxNumWorkers={{ .maxWorkers }}
          {{- end }}
          {{- if .useFlexRS }}
          - --flexRSGoal=COST_OPTIMIZED
          {{- else }}
          - --experiments=shuffle_mode=service
          {{- end }}
          {{- end }}

    ##
    ## Diff the bulk file-load requests staged in GCS against the files already present in the repo.
    ##
    - name: diff-file-loads
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: target-table
          {{- $targetTable := "{{inputs.parameters.target-table}}" }}
          - name: staging-bq-dataset
          {{- $bqDataset := "{{inputs.parameters.staging-bq-dataset}}" }}
      script:
        image: google/cloud-sdk:slim
        env:
          - name: GCS_PREFIX
            value: {{ printf "gs://%s/%s" .Values.gcs.stagingBucketName $gcsPrefix | quote }}
          - name: TARGET_TABLE
            value: {{ $targetTable | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ .Values.bigquery.jadeData.dataset }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/diff-data-files.sh") | indent 10 }}

    ##
    ## List the contents of a GCS bucket under a prefix, returning the results as a JSON array
    ## so Argo can parse and scatter over them.
    ##
    - name: list-gcs-contents
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/gcs-python:1.0.0
        env:
          - name: GCS_BUCKET
            value: {{ .Values.gcs.stagingBucketName }}
          - name: GCS_PREFIX
            value: {{ $gcsPrefix | quote }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/list-gcs-files-as-array.py") | indent 10 }}

    ##
    ## Run a repo bulk-file ingest to completion.
    ##
    - name: run-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: control-file-index
          {{- $controlFileIndex := "{{inputs.parameters.control-file-index}}" }}
          - name: load-tag-prefix
          {{- $loadTagPrefix := "{{inputs.parameters.load-tag-prefix}}" }}
          - name: total-file-count
          {{- $totalFileCount := "{{inputs.parameters.total-file-count}}" }}
      dag:
        tasks:
          - name: submit
            template: submit-bulk-file-ingest
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ $controlFilePath | quote }}
                - name: load-tag
                  value: {{ printf "%s-%s" $loadTagPrefix $controlFileIndex | quote }}
                - name: max-failures
                  value: {{ $totalFileCount | quote }}
          {{- $jobId := "{{tasks.submit.outputs.result}}" }}

          - name: poll
            dependencies: [submit]
            template: poll-ingest-job
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}
                {{- with .Values.repo }}
                - name: api-url
                  value: {{ .url }}
                - name: timeout
                  value: {{ .pollTimeout | quote }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: check
            dependencies: [poll]
            template: check-bulk-file-ingest
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}

    ##
    ## Submit a bulk-file ingest using a request staged in GCS.
    ##
    - name: submit-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: load-tag
          {{- $loadTag := "{{inputs.parameters.load-tag}}" }}
          - name: max-failures
          {{- $maxFailures := "{{inputs.parameters.max-failures}}" }}
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: INPUT_PATH
            value: {{ $controlFilePath | quote }}
          - name: LOAD_TAG
            value: {{ $loadTag | quote }}
          - name: MAX_FAILURES
            value: {{ $maxFailures | quote }}
          {{- with .Values.repo }}
          - name: API_URL
            value: {{ .url }}
          - name: DATASET_ID
            value: {{ .datasetId }}
          - name: PROFILE_ID
            value: {{ .profileId }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .accessKey.secretKey }}
          {{- end }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/submit-bulk-file-ingest.py") | indent 10 }}

    ##
    ## Inject common template used to poll TDR jobs.
    ##
    {{- include "argo.poll-ingest-job" . | indent 4 }}

    ##
    ## Check that a bulk-file ingest completed without any failures.
    ##
    - name: check-bulk-file-ingest
      inputs:
        parameters:
          - name: job-id
          {{- $jobId := "{{inputs.parameters.job-id}}" }}
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: JOB_ID
            value: {{ $jobId | quote }}
          - name: API_URL
            value: {{ .Values.repo.url }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .Values.repo.accessKey.secretKey }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/check-bulk-file-ingest-result.py") | indent 10 }}

    ##
    ## Ingest metadata staged in GCS into a TDR table.
    ##
    - name: ingest-metadata
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: staging-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-prefix}}" }}
          - name: metadata-type
          {{- $metadataType := "{{inputs.parameters.metadata-type}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
      dag:
        tasks:
          {{- $newRowsPrefix := printf "%s/new-rows/%s" $stagingPrefix $table }}
          {{- $oldIdsPrefix := printf "%s/old-ids/%s" $stagingPrefix $table }}
          - name: diff-table
            templateRef:
              name: {{ .Values.argoTemplates.diffBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: input-prefix
                  value: {{ printf "%s/%s/%s" $stagingPrefix $metadataType $table | quote }}
                - name: old-ids-output-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: new-rows-output-prefix
                  value: {{ $newRowsPrefix | quote }}
                - name: staging-bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: staging-bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: jade-bq-project
                  value: {{ .Values.bigquery.jadeData.project }}
                - name: jade-bq-dataset
                  value: {{ .Values.bigquery.jadeData.dataset }}
                - name: upsert
                  value: 'true'
                - name: diff-full-history
                  value: 'true'
            {{- $shouldAppend := "{{tasks.diff-table.outputs.parameters.rows-to-append-count}} > 0" }}
            {{- $shouldDelete := "{{tasks.diff-table.outputs.parameters.ids-to-delete-count}} > 0" }}

          - name: soft-delete-table
            dependencies: [diff-table]
            when: {{ $shouldDelete | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.softDeleteTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote}}
                - name: gcs-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                {{- with .Values.repo }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: ingest-table
            dependencies: [diff-table, soft-delete-table]
            when: {{ $shouldAppend | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.ingestTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote}}
                - name: gcs-prefix
                  value: {{ $newRowsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                {{- with .Values.repo }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

    ##
    ## Ingest metadata for a table containing information about data files.
    ##
    - name: ingest-file-metadata
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: staging-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-prefix}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
      dag:
        tasks:
          - name: inject-file-ids
            template: inject-file-ids
            arguments:
              parameters:
                - name: table
                  value: {{ $table | quote }}
                - name: gcs-prefix
                  value: {{ printf "%s/metadata" $stagingPrefix | quote }}
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
          {{- $fileIdsTable := "{{tasks.inject-file-ids.outputs.result}}" }}

          {{- $metadataPrefix := printf "%s/file-metadata-with-ids/%s" $stagingPrefix $table }}
          - name: export-metadata-with-ids
            dependencies: [inject-file-ids]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: bq-table
                  value: {{ $fileIdsTable | quote }}
                - name: output-format
                  value: NEWLINE_DELIMITED_JSON
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $metadataPrefix | quote }}
          {{- $rowCount := "{{tasks.export-metadata-with-ids.outputs.parameters.row-count}}" }}

          - name: ingest-metadata
            dependencies: [export-metadata-with-ids]
            when: {{ printf "%s > 0" $rowCount | quote }}
            template: ingest-metadata
            arguments:
              parameters:
                - name: table
                  value: {{ $table | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: metadata-type
                  value: file-metadata-with-ids
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}

    ##
    ## Rewrite staged tabular data for files to inject corresponding TDR file IDs.
    ##
    - name: inject-file-ids
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
      script:
        image: google/cloud-sdk:slim
        env:
          - name: GCS_PREFIX
            value: {{ printf "gs://%s/%s/%s" .Values.gcs.stagingBucketName $gcsPrefix $table | quote }}
          - name: TABLE
            value: {{ $table | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ .Values.bigquery.jadeData.dataset }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/inject-file-ids.sh") | indent 10 }}
