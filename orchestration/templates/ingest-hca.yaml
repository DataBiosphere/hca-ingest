apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ingest-hca
spec:
  entrypoint: main
  serviceAccountName: {{ .Values.serviceAccount.k8sName }}
  templates:
    - name: main
      inputs:
        parameters:
          - name: source-bucket-name
          {{- $inputBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
          {{- $inputPrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      dag:
        tasks:
          # Clear the staging directory to avoid corrupt state.
          - name: clear-staging-dir
            template: delete-gcs-directory
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $stagingPrefix | quote }}

          # Pre-process the metadata found in a source bucket
          - name: pre-process-metadata
            dependencies: [clear-staging-dir]
            template: run-dataflow
            arguments:
              parameters:
                - name: source-bucket-name
                  value: {{ $inputBucket | quote }}
                - name: source-bucket-prefix
                  value: {{ $inputPrefix | quote }}
                - name: staging-bucket-prefix
                  value: {{ $stagingPrefix | quote }}

          {{- $datasetName := printf "%s_%s" .Values.bigquery.stagingData.datasetPrefix $stagingPrefix }}
          - name: create-staging-dataset
            templateRef:
              name: {{ .Values.argoTemplates.createBQDataset.name }}
              template: main
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $datasetName | quote }}
                {{- with .Values.bigquery.stagingData }}
                - name: bq-project
                  value: {{ .project }}
                - name: dataset-description
                  value: {{ .description }}
                - name: dataset-expiration
                  value: {{ .expiration | quote }}
                {{- end }}

          {{- $inputFileLoadPrefix := printf "%s/data-transfer-requests" $stagingPrefix }}
          {{- $fileLoadTable := "file_load_requests" }}
          - name: diff-file-loads
            dependencies: [pre-process-metadata, create-staging-dataset]
            template: diff-file-loads
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $inputFileLoadPrefix | quote }}
                - name: target-table
                  value: {{ $fileLoadTable }}
                - name: staging-bq-dataset
                  value: {{ $datasetName | quote }}

          {{- $outputFileLoadPrefix := printf "%s/data-transfer-requests-deduped" $stagingPrefix }}
          - name: extract-file-loads
            dependencies: [diff-file-loads]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $datasetName | quote }}
                - name: bq-table
                  value: {{ $fileLoadTable }}
                - name: output-format
                  value: NEWLINE_DELIMITED_JSON
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
          {{- $totalLoadCount := "{{tasks.extract-file-loads.outputs.parameters.row-count}}" }}

          - name: list-deduped-requests
            dependencies: [extract-file-loads]
            template: list-gcs-contents
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $outputFileLoadPrefix | quote }}
            {{- $fileLoads := "{{tasks.list-deduped-requests.outputs.result}}" }}

          - name: ingest-data-files
            dependencies: [list-deduped-requests, extract-file-loads]
            template: run-bulk-file-ingest
            withParam: {{ $fileLoads | quote }}
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ "{{item.path}}" | quote }}
                - name: control-file-index
                  value: {{ "{{item.id}}" | quote }}
                - name: load-tag-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: total-file-count
                  value: {{ $totalLoadCount | quote }}

    ##
    ## Delete all the files under a prefix in GCS.
    ##
    - name: delete-gcs-directory
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      container:
        image: google/cloud-sdk:slim
        command: [gsutil, -m]
        args: [rm, -r, {{ printf "gs://%s/%s/*" .Values.gcs.stagingBucketName $gcsPrefix | quote }}]

    ##
    ## Run a Dataflow job to pre-process JSON metadata from the input area,
    ## moving it into our GCP space in the process.
    ##
    - name: run-dataflow
      inputs:
        parameters:
          - name: source-bucket-name
          {{- $sourceBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
          {{- $sourcePrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      container:
        {{- $version := default "latest" .Chart.AppVersion }}
        image: us.gcr.io/broad-dsp-gcr-public/hca-transformation-pipeline:{{ $version }}
        command: []
        args:
          - --runner=dataflow
          - --inputPrefix=gs://{{ $sourceBucket }}{{ $sourcePrefix }}
          - --outputPrefix=gs://{{ .Values.gcs.stagingBucketName }}/{{ $stagingPrefix }}
          {{- with .Values.dataflow }}
          - --project={{ .project }}
          - --region={{ .region }}
          - --tempLocation=gs://{{ .tmpBucketName }}/dataflow
          - --subnetwork=regions/{{ .region }}/subnetworks/{{ .subnetName }}
          - --serviceAccount={{ .workerAccount }}
          - --workerMachineType={{ .workerMachineType }}
          {{- with .autoscaling }}
          - --autoscalingAlgorithm=THROUGHPUT_BASED
          - --numWorkers={{ .minWorkers }}
          - --maxNumWorkers={{ .maxWorkers }}
          {{- end }}
          {{- if .useFlexRS }}
          - --flexRSGoal=COST_OPTIMIZED
          {{- else }}
          - --experiments=shuffle_mode=service
          {{- end }}
          {{- end }}

    ##
    ## Diff the bulk file-load requests staged in GCS against the files already present in the repo.
    ##
    - name: diff-file-loads
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
          - name: target-table
          {{- $targetTable := "{{inputs.parameters.target-table}}" }}
          - name: staging-bq-dataset
          {{- $bqDataset := "{{inputs.parameters.staging-bq-dataset}}" }}
      script:
        image: google/cloud-sdk:slim
        env:
          - name: GCS_PREFIX
            value: {{ printf "gs://%s/%s" .Values.gcs.stagingBucketName $gcsPrefix | quote }}
          - name: TARGET_TABLE
            value: {{ $targetTable | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ .Values.bigquery.jadeData.dataset }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/diff-data-files.sh") | indent 10 }}

    ##
    ## List the contents of a GCS bucket under a prefix, returning the results as a JSON array
    ## so Argo can parse and scatter over them.
    ##
    - name: list-gcs-contents
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/gcs-python:1.0.0
        env:
          - name: GCS_BUCKET
            value: {{ .Values.gcs.stagingBucketName }}
          - name: GCS_PREFIX
            value: {{ $gcsPrefix | quote }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/list-gcs-files-as-array.py") | indent 10 }}

    ##
    ## Run a repo bulk-file ingest to completion.
    ##
    - name: run-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: control-file-index
          {{- $controlFileIndex := "{{inputs.parameters.control-file-index}}" }}
          - name: load-tag-prefix
          {{- $loadTagPrefix := "{{inputs.parameters.load-tag-prefix}}" }}
          - name: total-file-count
          {{- $totalFileCount := "{{inputs.parameters.total-file-count}}" }}
      dag:
        tasks:
          - name: submit
            template: submit-bulk-file-ingest
            arguments:
              parameters:
                - name: control-file-path
                  value: {{ $controlFilePath | quote }}
                - name: load-tag
                  value: {{ printf "%s-%s" $loadTagPrefix $controlFileIndex | quote }}
                - name: max-failures
                  value: {{ $totalFileCount | quote }}
          {{- $jobId := "{{tasks.submit.outputs.result}}" }}

          - name: poll
            dependencies: [submit]
            template: poll-ingest-job
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}
                {{- with .Values.repo }}
                - name: api-url
                  value: {{ .url }}
                - name: timeout
                  value: {{ .pollTimeout | quote }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: check
            dependencies: [poll]
            template: check-bulk-file-ingest
            arguments:
              parameters:
                - name: job-id
                  value: {{ $jobId | quote }}

    ##
    ## Submit a bulk-file ingest using a request staged in GCS.
    ##
    - name: submit-bulk-file-ingest
      inputs:
        parameters:
          - name: control-file-path
          {{- $controlFilePath := "{{inputs.parameters.control-file-path}}" }}
          - name: load-tag
          {{- $loadTag := "{{inputs.parameters.load-tag}}" }}
          - name: max-failures
          {{- $maxFailures := "{{inputs.parameters.max-failures}}" }}
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: INPUT_PATH
            value: {{ $controlFilePath | quote }}
          - name: LOAD_TAG
            value: {{ $loadTag | quote }}
          - name: MAX_FAILURES
            value: {{ $maxFailures | quote }}
          {{- with .Values.repo }}
          - name: API_URL
            value: {{ .url }}
          - name: DATASET_ID
            value: {{ .datasetId }}
          - name: PROFILE_ID
            value: {{ .profileId }}
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: {{ printf "/secret/%s" .accessKey.secretKey }}
          {{- end }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/submit-bulk-file-ingest.py") | indent 10 }}

    ##
    ## Inject common template used to poll TDR jobs.
    ##
    {{- include "argo.poll-ingest-job" . | indent 4 }}

    ##
    ## Check that a bulk-file ingest completed without any failures.
    ##
    - name: check-bulk-file-ingest
      inputs:
        parameters:
          - name: job-id
          {{- $jobId := "{{inputs.parameters.job-id}}" }}
      volumes:
        - name: sa-secret-volume
          secret:
            secretName: {{ .Values.repo.accessKey.secretName }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/monster-auth-req-py:1.0.1
        volumeMounts:
          - name: sa-secret-volume
            mountPath: /secret
        env:
          - name: JOB_ID
            value: {{ $jobId | quote }}
          - name: API_URL
            value: {{ .Values.repo.url }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/check-bulk-file-ingest-result.py") | indent 10 }}
