apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: import-hca-no-files
spec:
  entrypoint: main
  serviceAccountName: {{ .Values.serviceAccount.k8sName }}
  arguments:
    parameters:
      - name: source-bucket-name
        value: src
      - name: source-bucket-prefix
        value: pre
      - name: staging-bucket-prefix
        value: stage
  templates:
    - name: main
      inputs:
        parameters:
          - name: source-bucket-name
            value: "{{workflow.parameters.source-bucket-name}}"
          {{- $inputBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
            value: "{{workflow.parameters.source-bucket-prefix}}"
          {{- $inputPrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
            value: "{{workflow.parameters.staging-bucket-prefix}}"
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      # Limit the number of active processing "arms" at a time.
      # There may be multiple pods running within each arm.
      parallelism: {{ .Values.parallelism }}
      dag:
        tasks:
          # Clear the staging directory to avoid corrupt state.
          - name: clear-staging-dir
            template: delete-gcs-directory
            arguments:
              parameters:
                - name: gcs-prefix
                  value: {{ $stagingPrefix | quote }}

          # Pre-process the metadata found in a source bucket
          - name: pre-process-metadata
            dependencies: [clear-staging-dir]
            template: run-dataflow
            arguments:
              parameters:
                - name: source-bucket-name
                  value: {{ $inputBucket | quote }}
                - name: source-bucket-prefix
                  value: {{ $inputPrefix | quote }}
                - name: staging-bucket-prefix
                  value: {{ $stagingPrefix | quote }}

          {{- $datasetName := printf "%s_%s" .Values.bigquery.stagingData.datasetPrefix $stagingPrefix }}
          - name: create-staging-dataset
            templateRef:
              name: {{ .Values.argoTemplates.createBQDataset.name }}
              template: main
            arguments:
              parameters:
                - name: dataset-name
                  value: {{ $datasetName | quote }}
                {{- with .Values.bigquery.stagingData }}
                - name: bq-project
                  value: {{ .project }}
                - name: dataset-description
                  value: {{ .description }}
                - name: dataset-expiration
                  value: {{ .expiration | quote }}
                {{- end }}

          - name: ingest-all-metadata
            dependencies: [pre-process-metadata, create-staging-dataset]
            withItems:
              - aggregate_generation_protocol
              - analysis_process
              - analysis_protocol
              - cell_line
              - cell_suspension
              - collection_protocol
              - differentiation_protocol
              - dissociation_protocol
              - donor_organism
              - enrichment_protocol
              - imaged_specimen
              - imaging_preparation_protocol
              - imaging_protocol
              - ipsc_induction_protocol
              - library_preparation_protocol
              - organoid
              - process
              - project
              - protocol
              - sequencing_protocol
              - specimen_from_organism
              - links
              - analysis_file
              - image_file
              - reference_file
              - sequence_file
              - supplementary_file
            template: ingest-metadata
            arguments:
              parameters:
                - name: table
                  value: {{ "{{item}}" | quote }}
                - name: staging-prefix
                  value: {{ $stagingPrefix | quote }}
                - name: metadata-type
                  value: metadata
                - name: bq-dataset
                  value: {{ $datasetName | quote }}

    ##
    ## Delete all the files under a prefix in GCS.
    ##
    - name: delete-gcs-directory
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: google/cloud-sdk:slim
        {{- $fullPath := printf "gs://%s/%s/*" .Values.gcs.stagingBucketName $gcsPrefix }}
        command: [bash]
        source: |
          # rm will fail if there's already nothing at the target path.
          gsutil -m rm -r {{ $fullPath }} || true

    ##
    ## Run a Dataflow job to pre-process JSON metadata from the input area,
    ## moving it into our GCP space in the process.
    ##
    - name: run-dataflow
      inputs:
        parameters:
          - name: source-bucket-name
          {{- $sourceBucket := "{{inputs.parameters.source-bucket-name}}" }}
          - name: source-bucket-prefix
          {{- $sourcePrefix := "{{inputs.parameters.source-bucket-prefix}}" }}
          - name: staging-bucket-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-bucket-prefix}}" }}
      container:
        {{- $version := default "latest" .Chart.AppVersion }}
        image: us.gcr.io/broad-dsp-gcr-public/hca-transformation-pipeline:{{ $version }}
        command: []
        args:
          - --runner=dataflow
          - --inputPrefix=gs://{{ $sourceBucket }}{{ $sourcePrefix }}
          - --outputPrefix=gs://{{ .Values.gcs.stagingBucketName }}/{{ $stagingPrefix }}
          {{- with .Values.dataflow }}
          - --project={{ .project }}
          - --region={{ .region }}
          - --tempLocation=gs://{{ .tmpBucketName }}/dataflow
          - --subnetwork=regions/{{ .region }}/subnetworks/{{ .subnetName }}
          - --serviceAccount={{ .workerAccount }}
          - --workerMachineType={{ .workerMachineType }}
          {{- with .autoscaling }}
          - --autoscalingAlgorithm=THROUGHPUT_BASED
          - --numWorkers={{ .minWorkers }}
          - --maxNumWorkers={{ .maxWorkers }}
          {{- end }}
          {{- if .useFlexRS }}
          - --flexRSGoal=COST_OPTIMIZED
          {{- else }}
          - --experiments=shuffle_mode=service
          {{- end }}
          {{- end }}


    ##
    ## List the contents of a GCS bucket under a prefix, returning the results as a JSON array
    ## so Argo can parse and scatter over them.
    ##
    - name: list-gcs-contents
      inputs:
        parameters:
          - name: gcs-prefix
          {{- $gcsPrefix := "{{inputs.parameters.gcs-prefix}}" }}
      script:
        image: us.gcr.io/broad-dsp-gcr-public/gcs-python:1.0.0
        env:
          - name: GCS_BUCKET
            value: {{ .Values.gcs.stagingBucketName }}
          - name: GCS_PREFIX
            value: {{ $gcsPrefix | quote }}
        command: [python]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/list-gcs-files-as-array.py") | indent 10 }}


    ##
    ## Inject common template used to poll TDR jobs.
    ##
    {{- include "argo.poll-ingest-job" . | indent 4 }}

    ##
    ## Ingest metadata staged in GCS into a TDR table.
    ##
    - name: ingest-metadata
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: staging-prefix
          {{- $stagingPrefix := "{{inputs.parameters.staging-prefix}}" }}
          - name: metadata-type
          {{- $metadataType := "{{inputs.parameters.metadata-type}}" }}
          - name: bq-dataset
          {{- $bqDataset := "{{inputs.parameters.bq-dataset}}" }}
      dag:
        tasks:
          {{- $newRowsPrefix := printf "%s/new-rows/%s" $stagingPrefix $table }}
          {{- $oldIdsPrefix := printf "%s/old-ids/%s" $stagingPrefix $table }}
          - name: diff-table
            templateRef:
              name: {{ .Values.argoTemplates.diffBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: input-prefix
                  value: {{ printf "%s/%s/%s" $stagingPrefix $metadataType $table | quote }}
                - name: old-ids-output-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: new-rows-output-prefix
                  value: {{ $newRowsPrefix | quote }}
                - name: staging-bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: staging-bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: jade-bq-project
                  value: {{ .Values.bigquery.jadeData.project }}
                - name: jade-bq-dataset
                  value: {{ .Values.bigquery.jadeData.dataset }}
                - name: upsert
                  value: 'true'
                - name: diff-full-history
                  value: 'true'
            {{- $shouldAppend := "{{tasks.diff-table.outputs.parameters.rows-to-append-count}} > 0" }}
            {{- $shouldDelete := "{{tasks.diff-table.outputs.parameters.ids-to-delete-count}} > 0" }}

          - name: soft-delete-table
            dependencies: [diff-table]
            when: {{ $shouldDelete | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.softDeleteTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote}}
                - name: gcs-prefix
                  value: {{ $oldIdsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                {{- with .Values.repo }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: ingest-table
            dependencies: [diff-table, soft-delete-table]
            when: {{ $shouldAppend | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.ingestTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote}}
                - name: gcs-prefix
                  value: {{ $newRowsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                {{- with .Values.repo }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

          - name: get-outdated-ids
            dependencies: [ingest-table]
            template: get-outdated-ids
            arguments:
              parameters:
                - name: table
                  value: {{ $table | quote }}
                - name: staging-bq-dataset
                  value: {{ $bqDataset | quote }}
            {{- $outdatedIdsTable := "{{tasks.get-outdated-ids.outputs.result}}" }}

          {{- $outdatedIdsPrefix := printf "%s/outdated-ids/%s" $stagingPrefix $table }}
          - name: export-outdated
            dependencies: [get-outdated-ids]
            templateRef:
              name: {{ .Values.argoTemplates.exportBQTable.name }}
              template: main
            arguments:
              parameters:
                - name: bq-project
                  value: {{ .Values.bigquery.stagingData.project }}
                - name: bq-dataset
                  value: {{ $bqDataset | quote }}
                - name: bq-table
                  value: {{ $outdatedIdsTable | quote }}
                - name: output-format
                  value: CSV
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                - name: gcs-prefix
                  value: {{ $outdatedIdsPrefix | quote }}
            {{- $outdatedCount := "{{tasks.export-outdated.outputs.parameters.row-count}}" }}

          - name: soft-delete-outdated
            dependencies: [export-outdated]
            when: {{ printf "%s > 0" $outdatedCount | quote }}
            templateRef:
              name: {{ .Values.argoTemplates.softDeleteTable.name }}
              template: main
            arguments:
              parameters:
                - name: table-name
                  value: {{ $table | quote}}
                - name: gcs-prefix
                  value: {{ $outdatedIdsPrefix | quote }}
                - name: gcs-bucket
                  value: {{ .Values.gcs.stagingBucketName }}
                {{- with .Values.repo }}
                - name: url
                  value: {{ .url }}
                - name: dataset-id
                  value: {{ .datasetId }}
                - name: timeout
                  value: {{ .pollTimeout }}
                - name: sa-secret
                  value: {{ .accessKey.secretName }}
                - name: sa-secret-key
                  value: {{ .accessKey.secretKey }}
                {{- end }}

    ##
    ## Extract the TDR row IDs for rows containing an "outdated" version of a document,
    ## where "outdated" is determined by the version of other data stored in the TDR dataset.
    ##
    - name: get-outdated-ids
      inputs:
        parameters:
          - name: table
          {{- $table := "{{inputs.parameters.table}}" }}
          - name: staging-bq-dataset
          {{- $bqDataset := "{{inputs.parameters.staging-bq-dataset}}" }}
      script:
        image: google/cloud-sdk:slim
        env:
          - name: TABLE
            value: {{ $table | quote }}
          - name: STAGING_PROJECT
            value: {{ .Values.bigquery.stagingData.project }}
          - name: STAGING_DATASET
            value: {{ $bqDataset | quote }}
          - name: JADE_PROJECT
            value: {{ .Values.bigquery.jadeData.project }}
          - name: JADE_DATASET
            value: {{ .Values.bigquery.jadeData.dataset }}
        command: [bash]
        source: |
        {{- include "argo.render-lines" (.Files.Lines "scripts/get-outdated-row-ids.sh") | indent 10 }}
